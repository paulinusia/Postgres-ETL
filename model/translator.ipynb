{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = pd.read_table('./pol-eng/pol.txt', names=['eng', 'pol','idx'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38699, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_eng_words = set()\n",
    "all_pol_words = set()\n",
    "length_list_eng = []\n",
    "length_list_pol = []\n",
    "txt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the lambda keyword to declare an anonymous function, which is why we refer to them as \"lambda functions\". \n",
    "#An anonymous function refers to a function declared with no name.\n",
    "txt.pol = txt.pol.apply(lambda x: x.lower())\n",
    "txt.eng = txt.eng.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>pol</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>go.</td>\n",
       "      <td>idź.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hi.</td>\n",
       "      <td>cześć.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>run!</td>\n",
       "      <td>uciekaj!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>run.</td>\n",
       "      <td>biegnij.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>run.</td>\n",
       "      <td>uciekaj.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38694</td>\n",
       "      <td>no matter how much you try to convince people ...</td>\n",
       "      <td>nieważne, jak bardzo usiłujesz przekonać ludzi...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38695</td>\n",
       "      <td>a child who is a native speaker usually knows ...</td>\n",
       "      <td>dziecko zwykle wie o swoim języku ojczystym rz...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38696</td>\n",
       "      <td>since there are usually multiple websites on a...</td>\n",
       "      <td>zwykle jest wiele stron internetowych na każdy...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38697</td>\n",
       "      <td>if you want to sound like a native speaker, yo...</td>\n",
       "      <td>jeśli chcesz mówić jak rodzimy użytkownik, mus...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38698</td>\n",
       "      <td>if someone who doesn't know your background sa...</td>\n",
       "      <td>jeśli ktoś, kto nas nie zna, mówi, że mówimy j...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38699 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "0                                                    go.   \n",
       "1                                                    hi.   \n",
       "2                                                   run!   \n",
       "3                                                   run.   \n",
       "4                                                   run.   \n",
       "...                                                  ...   \n",
       "38694  no matter how much you try to convince people ...   \n",
       "38695  a child who is a native speaker usually knows ...   \n",
       "38696  since there are usually multiple websites on a...   \n",
       "38697  if you want to sound like a native speaker, yo...   \n",
       "38698  if someone who doesn't know your background sa...   \n",
       "\n",
       "                                                     pol  \\\n",
       "0                                                   idź.   \n",
       "1                                                 cześć.   \n",
       "2                                               uciekaj!   \n",
       "3                                               biegnij.   \n",
       "4                                               uciekaj.   \n",
       "...                                                  ...   \n",
       "38694  nieważne, jak bardzo usiłujesz przekonać ludzi...   \n",
       "38695  dziecko zwykle wie o swoim języku ojczystym rz...   \n",
       "38696  zwykle jest wiele stron internetowych na każdy...   \n",
       "38697  jeśli chcesz mówić jak rodzimy użytkownik, mus...   \n",
       "38698  jeśli ktoś, kto nas nie zna, mówi, że mówimy j...   \n",
       "\n",
       "                                                     idx  \n",
       "0      CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1      CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "2      CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "3      CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "4      CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "...                                                  ...  \n",
       "38694  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "38695  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "38696  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "38697  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "38698  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "\n",
       "[38699 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.drop('idx', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>pol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>go.</td>\n",
       "      <td>idź.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hi.</td>\n",
       "      <td>cześć.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>run!</td>\n",
       "      <td>uciekaj!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>run.</td>\n",
       "      <td>biegnij.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>run.</td>\n",
       "      <td>uciekaj.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38694</td>\n",
       "      <td>no matter how much you try to convince people ...</td>\n",
       "      <td>nieważne, jak bardzo usiłujesz przekonać ludzi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38695</td>\n",
       "      <td>a child who is a native speaker usually knows ...</td>\n",
       "      <td>dziecko zwykle wie o swoim języku ojczystym rz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38696</td>\n",
       "      <td>since there are usually multiple websites on a...</td>\n",
       "      <td>zwykle jest wiele stron internetowych na każdy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38697</td>\n",
       "      <td>if you want to sound like a native speaker, yo...</td>\n",
       "      <td>jeśli chcesz mówić jak rodzimy użytkownik, mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38698</td>\n",
       "      <td>if someone who doesn't know your background sa...</td>\n",
       "      <td>jeśli ktoś, kto nas nie zna, mówi, że mówimy j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38699 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "0                                                    go.   \n",
       "1                                                    hi.   \n",
       "2                                                   run!   \n",
       "3                                                   run.   \n",
       "4                                                   run.   \n",
       "...                                                  ...   \n",
       "38694  no matter how much you try to convince people ...   \n",
       "38695  a child who is a native speaker usually knows ...   \n",
       "38696  since there are usually multiple websites on a...   \n",
       "38697  if you want to sound like a native speaker, yo...   \n",
       "38698  if someone who doesn't know your background sa...   \n",
       "\n",
       "                                                     pol  \n",
       "0                                                   idź.  \n",
       "1                                                 cześć.  \n",
       "2                                               uciekaj!  \n",
       "3                                               biegnij.  \n",
       "4                                               uciekaj.  \n",
       "...                                                  ...  \n",
       "38694  nieważne, jak bardzo usiłujesz przekonać ludzi...  \n",
       "38695  dziecko zwykle wie o swoim języku ojczystym rz...  \n",
       "38696  zwykle jest wiele stron internetowych na każdy...  \n",
       "38697  jeśli chcesz mówić jak rodzimy użytkownik, mus...  \n",
       "38698  jeśli ktoś, kto nas nie zna, mówi, że mówimy j...  \n",
       "\n",
       "[38699 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quote removal\n",
    "txt.eng = txt.eng.apply(lambda x: re.sub(\"''\", '', x))\n",
    "txt.pol = txt.pol.apply(lambda x: re.sub(\"''\", '', x))\n",
    "#special char\n",
    "txt.eng = txt.eng.apply(lambda x: re.sub(\"[|\\^&+\\-%*/=!>]\", '', x))\n",
    "txt.pol = txt.pol.apply(lambda x: re.sub(\"[|\\^&+\\-%*/=!>]\", '', x))\n",
    "\n",
    "#punctuation\n",
    "txt.eng = txt.eng.apply(lambda x: re.sub(r'[^\\w\\s]', \"\", x))\n",
    "txt.pol = txt.pol.apply(lambda x: re.sub(r'[^\\w\\s]', \"\", x))\n",
    "\n",
    "#numbers\n",
    "txt.eng = txt.eng.apply(lambda x: re.sub(\"[0-9]\", '', x))\n",
    "txt.pol = txt.pol.apply(lambda x: re.sub(\"[0-9]\", '', x))\n",
    "\n",
    "#extra spacing\n",
    "txt.eng = txt.eng.replace(\"/^\\s+|\\s+$|\\s+(?=\\s)/g\", \"\")\n",
    "txt.pol = txt.pol.replace(\"/^\\s+|\\s+$|\\s+(?=\\s)/g\", \"\")\n",
    "\n",
    "#Start and end tokens ?\n",
    "\n",
    "txt.pol = txt.pol.apply(lambda x : 'START_' + x + '_END')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>pol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "      <td>START_idź_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hi</td>\n",
       "      <td>START_cześć_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>run</td>\n",
       "      <td>START_uciekaj_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>run</td>\n",
       "      <td>START_biegnij_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>run</td>\n",
       "      <td>START_uciekaj_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38694</td>\n",
       "      <td>no matter how much you try to convince people ...</td>\n",
       "      <td>START_nieważne jak bardzo usiłujesz przekonać ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38695</td>\n",
       "      <td>a child who is a native speaker usually knows ...</td>\n",
       "      <td>START_dziecko zwykle wie o swoim języku ojczys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38696</td>\n",
       "      <td>since there are usually multiple websites on a...</td>\n",
       "      <td>START_zwykle jest wiele stron internetowych na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38697</td>\n",
       "      <td>if you want to sound like a native speaker you...</td>\n",
       "      <td>START_jeśli chcesz mówić jak rodzimy użytkowni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38698</td>\n",
       "      <td>if someone who doesnt know your background say...</td>\n",
       "      <td>START_jeśli ktoś kto nas nie zna mówi że mówim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38699 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "0                                                     go   \n",
       "1                                                     hi   \n",
       "2                                                    run   \n",
       "3                                                    run   \n",
       "4                                                    run   \n",
       "...                                                  ...   \n",
       "38694  no matter how much you try to convince people ...   \n",
       "38695  a child who is a native speaker usually knows ...   \n",
       "38696  since there are usually multiple websites on a...   \n",
       "38697  if you want to sound like a native speaker you...   \n",
       "38698  if someone who doesnt know your background say...   \n",
       "\n",
       "                                                     pol  \n",
       "0                                          START_idź_END  \n",
       "1                                        START_cześć_END  \n",
       "2                                      START_uciekaj_END  \n",
       "3                                      START_biegnij_END  \n",
       "4                                      START_uciekaj_END  \n",
       "...                                                  ...  \n",
       "38694  START_nieważne jak bardzo usiłujesz przekonać ...  \n",
       "38695  START_dziecko zwykle wie o swoim języku ojczys...  \n",
       "38696  START_zwykle jest wiele stron internetowych na...  \n",
       "38697  START_jeśli chcesz mówić jak rodzimy użytkowni...  \n",
       "38698  START_jeśli ktoś kto nas nie zna mówi że mówim...  \n",
       "\n",
       "[38699 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete Vocab sizes for eng and pol\n",
    "#compute the vocab sizes and the length of max sequence for both the languages\n",
    "#create 4 python dicts (2 for each lang) to convert a given token into an integer index and vice versa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng in txt.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pol in txt.pol:\n",
    "    for word in pol.split():\n",
    "        if word not in all_pol_words:\n",
    "            all_pol_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the maximum length of the target sequence\n",
    "for l in txt.eng:\n",
    "    length_list_eng.append(len(l.split(' ')))\n",
    "    \n",
    "max_length_src = np.max(length_list_eng)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the maximum length of the target sequence\n",
    "for l in txt.pol:\n",
    "    length_list_pol.append(len(l.split(' ')))\n",
    "    \n",
    "max_length_tar = np.max(length_list_pol)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort dictionaries\n",
    "\n",
    "source_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_pol_words))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8201, 28427)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocab sizes\n",
    "#increase num_encoder_token by 1, that is num_encoder_token -. vocab_size + 1. \n",
    "\n",
    "num_encoder_tokens = len(all_eng_words)+1\n",
    "num_decoder_tokens = len(all_pol_words)\n",
    "num_decoder_tokens += 1 #padding\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD TO TOKEN DICTIONARY\n",
    "\n",
    "source_token_index = dict([(word, i+1) for i, word in enumerate(source_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'abandon': 2,\n",
       " 'abandoned': 3,\n",
       " 'abducted': 4,\n",
       " 'abhor': 5,\n",
       " 'abilities': 6,\n",
       " 'ability': 7,\n",
       " 'able': 8,\n",
       " 'aboard': 9,\n",
       " 'abolish': 10,\n",
       " 'abolished': 11,\n",
       " 'about': 12,\n",
       " 'above': 13,\n",
       " 'abraham': 14,\n",
       " 'abreast': 15,\n",
       " 'abroad': 16,\n",
       " 'abrupt': 17,\n",
       " 'absence': 18,\n",
       " 'absent': 19,\n",
       " 'absolutely': 20,\n",
       " 'absorb': 21,\n",
       " 'absorbed': 22,\n",
       " 'absorbs': 23,\n",
       " 'abstain': 24,\n",
       " 'abstract': 25,\n",
       " 'absurd': 26,\n",
       " 'abundant': 27,\n",
       " 'abused': 28,\n",
       " 'abuses': 29,\n",
       " 'academic': 30,\n",
       " 'accelerated': 31,\n",
       " 'accent': 32,\n",
       " 'accept': 33,\n",
       " 'accepted': 34,\n",
       " 'access': 35,\n",
       " 'accessible': 36,\n",
       " 'accident': 37,\n",
       " 'accidental': 38,\n",
       " 'accidentally': 39,\n",
       " 'accidents': 40,\n",
       " 'acclimated': 41,\n",
       " 'accompanied': 42,\n",
       " 'accompany': 43,\n",
       " 'accomplish': 44,\n",
       " 'accomplished': 45,\n",
       " 'accomplishes': 46,\n",
       " 'according': 47,\n",
       " 'accordion': 48,\n",
       " 'account': 49,\n",
       " 'accountable': 50,\n",
       " 'accountant': 51,\n",
       " 'accounted': 52,\n",
       " 'accounting': 53,\n",
       " 'accounts': 54,\n",
       " 'accumulated': 55,\n",
       " 'accuracy': 56,\n",
       " 'accurate': 57,\n",
       " 'accusations': 58,\n",
       " 'accused': 59,\n",
       " 'accusing': 60,\n",
       " 'accustom': 61,\n",
       " 'accustomed': 62,\n",
       " 'ace': 63,\n",
       " 'ache': 64,\n",
       " 'ached': 65,\n",
       " 'aches': 66,\n",
       " 'achieve': 67,\n",
       " 'achieved': 68,\n",
       " 'achievement': 69,\n",
       " 'achievements': 70,\n",
       " 'achille': 71,\n",
       " 'acid': 72,\n",
       " 'acids': 73,\n",
       " 'acknowledged': 74,\n",
       " 'acorn': 75,\n",
       " 'acquaintances': 76,\n",
       " 'acquainted': 77,\n",
       " 'acquired': 78,\n",
       " 'across': 79,\n",
       " 'act': 80,\n",
       " 'acted': 81,\n",
       " 'acting': 82,\n",
       " 'action': 83,\n",
       " 'actions': 84,\n",
       " 'active': 85,\n",
       " 'activity': 86,\n",
       " 'actor': 87,\n",
       " 'actors': 88,\n",
       " 'actress': 89,\n",
       " 'acts': 90,\n",
       " 'actually': 91,\n",
       " 'acupuncture': 92,\n",
       " 'acute': 93,\n",
       " 'adapt': 94,\n",
       " 'add': 95,\n",
       " 'added': 96,\n",
       " 'addict': 97,\n",
       " 'addicted': 98,\n",
       " 'adding': 99,\n",
       " 'addition': 100,\n",
       " 'additional': 101,\n",
       " 'additions': 102,\n",
       " 'address': 103,\n",
       " 'addressed': 104,\n",
       " 'addresses': 105,\n",
       " 'adequate': 106,\n",
       " 'adherents': 107,\n",
       " 'adjust': 108,\n",
       " 'adjusted': 109,\n",
       " 'admire': 110,\n",
       " 'admired': 111,\n",
       " 'admires': 112,\n",
       " 'admiring': 113,\n",
       " 'admit': 114,\n",
       " 'admitted': 115,\n",
       " 'admittedly': 116,\n",
       " 'adopt': 117,\n",
       " 'adopted': 118,\n",
       " 'adorable': 119,\n",
       " 'adore': 120,\n",
       " 'adores': 121,\n",
       " 'adult': 122,\n",
       " 'adults': 123,\n",
       " 'advance': 124,\n",
       " 'advanced': 125,\n",
       " 'advances': 126,\n",
       " 'advantage': 127,\n",
       " 'advantages': 128,\n",
       " 'adventure': 129,\n",
       " 'adventures': 130,\n",
       " 'adventurous': 131,\n",
       " 'adversity': 132,\n",
       " 'advertisement': 133,\n",
       " 'advertising': 134,\n",
       " 'advice': 135,\n",
       " 'advise': 136,\n",
       " 'advised': 137,\n",
       " 'advises': 138,\n",
       " 'aerobics': 139,\n",
       " 'affair': 140,\n",
       " 'affairs': 141,\n",
       " 'affect': 142,\n",
       " 'affectations': 143,\n",
       " 'affected': 144,\n",
       " 'afford': 145,\n",
       " 'afraid': 146,\n",
       " 'africa': 147,\n",
       " 'africanamericans': 148,\n",
       " 'after': 149,\n",
       " 'afternoon': 150,\n",
       " 'afternoons': 151,\n",
       " 'again': 152,\n",
       " 'against': 153,\n",
       " 'age': 154,\n",
       " 'aged': 155,\n",
       " 'agency': 156,\n",
       " 'agenda': 157,\n",
       " 'agent': 158,\n",
       " 'ages': 159,\n",
       " 'aggressive': 160,\n",
       " 'agitated': 161,\n",
       " 'ago': 162,\n",
       " 'agree': 163,\n",
       " 'agreeable': 164,\n",
       " 'agreed': 165,\n",
       " 'agreement': 166,\n",
       " 'agrees': 167,\n",
       " 'agriculture': 168,\n",
       " 'ahead': 169,\n",
       " 'ai': 170,\n",
       " 'aid': 171,\n",
       " 'aikido': 172,\n",
       " 'aim': 173,\n",
       " 'aimed': 174,\n",
       " 'aiming': 175,\n",
       " 'aimlessly': 176,\n",
       " 'aims': 177,\n",
       " 'aint': 178,\n",
       " 'air': 179,\n",
       " 'aires': 180,\n",
       " 'airline': 181,\n",
       " 'airplane': 182,\n",
       " 'airplanes': 183,\n",
       " 'airport': 184,\n",
       " 'airraid': 185,\n",
       " 'airtight': 186,\n",
       " 'akita': 187,\n",
       " 'alarm': 188,\n",
       " 'alarmed': 189,\n",
       " 'alaska': 190,\n",
       " 'album': 191,\n",
       " 'alcohol': 192,\n",
       " 'alcoholic': 193,\n",
       " 'alcoholism': 194,\n",
       " 'alert': 195,\n",
       " 'alexander': 196,\n",
       " 'algebra': 197,\n",
       " 'alibi': 198,\n",
       " 'alibis': 199,\n",
       " 'alice': 200,\n",
       " 'alien': 201,\n",
       " 'aliens': 202,\n",
       " 'alike': 203,\n",
       " 'alive': 204,\n",
       " 'alkalis': 205,\n",
       " 'all': 206,\n",
       " 'allergic': 207,\n",
       " 'allergies': 208,\n",
       " 'alleviate': 209,\n",
       " 'alley': 210,\n",
       " 'alliance': 211,\n",
       " 'allied': 212,\n",
       " 'alligators': 213,\n",
       " 'allow': 214,\n",
       " 'allowance': 215,\n",
       " 'allowed': 216,\n",
       " 'allyoucaneat': 217,\n",
       " 'almost': 218,\n",
       " 'alone': 219,\n",
       " 'along': 220,\n",
       " 'aloud': 221,\n",
       " 'alphabet': 222,\n",
       " 'alphabetically': 223,\n",
       " 'already': 224,\n",
       " 'alright': 225,\n",
       " 'also': 226,\n",
       " 'altercation': 227,\n",
       " 'alternative': 228,\n",
       " 'although': 229,\n",
       " 'altogether': 230,\n",
       " 'always': 231,\n",
       " 'am': 232,\n",
       " 'amassed': 233,\n",
       " 'amateur': 234,\n",
       " 'amateurs': 235,\n",
       " 'amaze': 236,\n",
       " 'amazed': 237,\n",
       " 'amazing': 238,\n",
       " 'ambassador': 239,\n",
       " 'ambassadors': 240,\n",
       " 'ambition': 241,\n",
       " 'ambitious': 242,\n",
       " 'ambulance': 243,\n",
       " 'ambush': 244,\n",
       " 'america': 245,\n",
       " 'american': 246,\n",
       " 'americans': 247,\n",
       " 'ammunition': 248,\n",
       " 'amnesty': 249,\n",
       " 'among': 250,\n",
       " 'amount': 251,\n",
       " 'amounted': 252,\n",
       " 'ample': 253,\n",
       " 'amused': 254,\n",
       " 'amusing': 255,\n",
       " 'an': 256,\n",
       " 'anaconda': 257,\n",
       " 'analysis': 258,\n",
       " 'anarchy': 259,\n",
       " 'anas': 260,\n",
       " 'ancestors': 261,\n",
       " 'ancient': 262,\n",
       " 'and': 263,\n",
       " 'anderson': 264,\n",
       " 'anemic': 265,\n",
       " 'angel': 266,\n",
       " 'angeles': 267,\n",
       " 'angels': 268,\n",
       " 'anger': 269,\n",
       " 'angkor': 270,\n",
       " 'angles': 271,\n",
       " 'angrily': 272,\n",
       " 'angry': 273,\n",
       " 'animal': 274,\n",
       " 'animals': 275,\n",
       " 'ankle': 276,\n",
       " 'anniversary': 277,\n",
       " 'announced': 278,\n",
       " 'announcer': 279,\n",
       " 'annoy': 280,\n",
       " 'annoyed': 281,\n",
       " 'annoying': 282,\n",
       " 'annoys': 283,\n",
       " 'annual': 284,\n",
       " 'annually': 285,\n",
       " 'another': 286,\n",
       " 'answer': 287,\n",
       " 'answered': 288,\n",
       " 'answering': 289,\n",
       " 'answers': 290,\n",
       " 'antagonize': 291,\n",
       " 'anthropologist': 292,\n",
       " 'antibiotics': 293,\n",
       " 'anticipated': 294,\n",
       " 'antiitch': 295,\n",
       " 'antimatter': 296,\n",
       " 'antique': 297,\n",
       " 'antiques': 298,\n",
       " 'anxious': 299,\n",
       " 'any': 300,\n",
       " 'anybody': 301,\n",
       " 'anybodys': 302,\n",
       " 'anyhow': 303,\n",
       " 'anymore': 304,\n",
       " 'anyone': 305,\n",
       " 'anyones': 306,\n",
       " 'anything': 307,\n",
       " 'anytime': 308,\n",
       " 'anyway': 309,\n",
       " 'anyways': 310,\n",
       " 'anywhere': 311,\n",
       " 'apart': 312,\n",
       " 'apartment': 313,\n",
       " 'apartments': 314,\n",
       " 'apes': 315,\n",
       " 'apologize': 316,\n",
       " 'apologized': 317,\n",
       " 'apologizing': 318,\n",
       " 'apology': 319,\n",
       " 'app': 320,\n",
       " 'apparent': 321,\n",
       " 'apparently': 322,\n",
       " 'appear': 323,\n",
       " 'appearance': 324,\n",
       " 'appeared': 325,\n",
       " 'appears': 326,\n",
       " 'appendicitis': 327,\n",
       " 'appendix': 328,\n",
       " 'appetite': 329,\n",
       " 'applauded': 330,\n",
       " 'applauding': 331,\n",
       " 'apple': 332,\n",
       " 'apples': 333,\n",
       " 'appliances': 334,\n",
       " 'applicants': 335,\n",
       " 'application': 336,\n",
       " 'applications': 337,\n",
       " 'applied': 338,\n",
       " 'apply': 339,\n",
       " 'applying': 340,\n",
       " 'appointed': 341,\n",
       " 'appointment': 342,\n",
       " 'appreciate': 343,\n",
       " 'appreciated': 344,\n",
       " 'appreciates': 345,\n",
       " 'appreciation': 346,\n",
       " 'approach': 347,\n",
       " 'approached': 348,\n",
       " 'approaches': 349,\n",
       " 'approaching': 350,\n",
       " 'appropriate': 351,\n",
       " 'approve': 352,\n",
       " 'approved': 353,\n",
       " 'approves': 354,\n",
       " 'approving': 355,\n",
       " 'approximately': 356,\n",
       " 'apps': 357,\n",
       " 'april': 358,\n",
       " 'arabia': 359,\n",
       " 'arabic': 360,\n",
       " 'arched': 361,\n",
       " 'archer': 362,\n",
       " 'architect': 363,\n",
       " 'are': 364,\n",
       " 'area': 365,\n",
       " 'areas': 366,\n",
       " 'arent': 367,\n",
       " 'argue': 368,\n",
       " 'argued': 369,\n",
       " 'argues': 370,\n",
       " 'arguing': 371,\n",
       " 'argument': 372,\n",
       " 'arguments': 373,\n",
       " 'arisen': 374,\n",
       " 'arithmetic': 375,\n",
       " 'arizona': 376,\n",
       " 'arm': 377,\n",
       " 'armchair': 378,\n",
       " 'armed': 379,\n",
       " 'armpits': 380,\n",
       " 'arms': 381,\n",
       " 'armstrong': 382,\n",
       " 'army': 383,\n",
       " 'around': 384,\n",
       " 'aroused': 385,\n",
       " 'arranged': 386,\n",
       " 'arrangement': 387,\n",
       " 'arranging': 388,\n",
       " 'arrest': 389,\n",
       " 'arrested': 390,\n",
       " 'arrival': 391,\n",
       " 'arrive': 392,\n",
       " 'arrived': 393,\n",
       " 'arrives': 394,\n",
       " 'arriving': 395,\n",
       " 'arrogance': 396,\n",
       " 'arrogant': 397,\n",
       " 'art': 398,\n",
       " 'arthropods': 399,\n",
       " 'article': 400,\n",
       " 'articles': 401,\n",
       " 'articulate': 402,\n",
       " 'artificial': 403,\n",
       " 'artist': 404,\n",
       " 'artistic': 405,\n",
       " 'artists': 406,\n",
       " 'as': 407,\n",
       " 'ashamed': 408,\n",
       " 'ashes': 409,\n",
       " 'ashtray': 410,\n",
       " 'asia': 411,\n",
       " 'aside': 412,\n",
       " 'ask': 413,\n",
       " 'asked': 414,\n",
       " 'asking': 415,\n",
       " 'asks': 416,\n",
       " 'asleep': 417,\n",
       " 'asparagus': 418,\n",
       " 'aspects': 419,\n",
       " 'aspirin': 420,\n",
       " 'aspirins': 421,\n",
       " 'assassinated': 422,\n",
       " 'assembled': 423,\n",
       " 'assembles': 424,\n",
       " 'assembly': 425,\n",
       " 'asserted': 426,\n",
       " 'assertive': 427,\n",
       " 'asserts': 428,\n",
       " 'assignment': 429,\n",
       " 'assist': 430,\n",
       " 'assistance': 431,\n",
       " 'assistant': 432,\n",
       " 'assistants': 433,\n",
       " 'associate': 434,\n",
       " 'assume': 435,\n",
       " 'assumed': 436,\n",
       " 'assumption': 437,\n",
       " 'assure': 438,\n",
       " 'assured': 439,\n",
       " 'asthma': 440,\n",
       " 'astrology': 441,\n",
       " 'astronaut': 442,\n",
       " 'astronomer': 443,\n",
       " 'astronomy': 444,\n",
       " 'astrophysicist': 445,\n",
       " 'at': 446,\n",
       " 'ate': 447,\n",
       " 'athens': 448,\n",
       " 'athletic': 449,\n",
       " 'atlas': 450,\n",
       " 'atm': 451,\n",
       " 'atmosphere': 452,\n",
       " 'atomic': 453,\n",
       " 'atoms': 454,\n",
       " 'atone': 455,\n",
       " 'attach': 456,\n",
       " 'attached': 457,\n",
       " 'attachments': 458,\n",
       " 'attack': 459,\n",
       " 'attacked': 460,\n",
       " 'attacking': 461,\n",
       " 'attacks': 462,\n",
       " 'attempt': 463,\n",
       " 'attempted': 464,\n",
       " 'attend': 465,\n",
       " 'attended': 466,\n",
       " 'attending': 467,\n",
       " 'attends': 468,\n",
       " 'attention': 469,\n",
       " 'attentive': 470,\n",
       " 'attic': 471,\n",
       " 'attitude': 472,\n",
       " 'attorney': 473,\n",
       " 'attract': 474,\n",
       " 'attracted': 475,\n",
       " 'attraction': 476,\n",
       " 'attractive': 477,\n",
       " 'attributed': 478,\n",
       " 'auction': 479,\n",
       " 'audacity': 480,\n",
       " 'audience': 481,\n",
       " 'audio': 482,\n",
       " 'audiobooks': 483,\n",
       " 'auditorium': 484,\n",
       " 'august': 485,\n",
       " 'aunt': 486,\n",
       " 'aunts': 487,\n",
       " 'australia': 488,\n",
       " 'australian': 489,\n",
       " 'austrian': 490,\n",
       " 'author': 491,\n",
       " 'authority': 492,\n",
       " 'autism': 493,\n",
       " 'auto': 494,\n",
       " 'autographs': 495,\n",
       " 'automatic': 496,\n",
       " 'automatically': 497,\n",
       " 'automobile': 498,\n",
       " 'autumn': 499,\n",
       " 'available': 500,\n",
       " 'avatar': 501,\n",
       " 'avenged': 502,\n",
       " 'average': 503,\n",
       " 'avid': 504,\n",
       " 'avoid': 505,\n",
       " 'avoided': 506,\n",
       " 'avoiding': 507,\n",
       " 'awaiting': 508,\n",
       " 'awake': 509,\n",
       " 'awaken': 510,\n",
       " 'award': 511,\n",
       " 'aware': 512,\n",
       " 'away': 513,\n",
       " 'awesome': 514,\n",
       " 'awful': 515,\n",
       " 'awfully': 516,\n",
       " 'awkward': 517,\n",
       " 'awoke': 518,\n",
       " 'ax': 519,\n",
       " 'b': 520,\n",
       " 'babbling': 521,\n",
       " 'baboons': 522,\n",
       " 'baby': 523,\n",
       " 'babysit': 524,\n",
       " 'babysitter': 525,\n",
       " 'babysitters': 526,\n",
       " 'bach': 527,\n",
       " 'bachelor': 528,\n",
       " 'bachelors': 529,\n",
       " 'back': 530,\n",
       " 'backed': 531,\n",
       " 'backfired': 532,\n",
       " 'background': 533,\n",
       " 'backgrounds': 534,\n",
       " 'backpack': 535,\n",
       " 'backyard': 536,\n",
       " 'bacteria': 537,\n",
       " 'bad': 538,\n",
       " 'badgers': 539,\n",
       " 'badly': 540,\n",
       " 'badmannered': 541,\n",
       " 'badminton': 542,\n",
       " 'baffled': 543,\n",
       " 'bag': 544,\n",
       " 'baggage': 545,\n",
       " 'bags': 546,\n",
       " 'baikal': 547,\n",
       " 'bail': 548,\n",
       " 'baked': 549,\n",
       " 'bakery': 550,\n",
       " 'baking': 551,\n",
       " 'balance': 552,\n",
       " 'balanced': 553,\n",
       " 'balcony': 554,\n",
       " 'bald': 555,\n",
       " 'balkan': 556,\n",
       " 'ball': 557,\n",
       " 'ballerina': 558,\n",
       " 'ballet': 559,\n",
       " 'balloon': 560,\n",
       " 'balls': 561,\n",
       " 'bambury': 562,\n",
       " 'banana': 563,\n",
       " 'bananas': 564,\n",
       " 'band': 565,\n",
       " 'bands': 566,\n",
       " 'bang': 567,\n",
       " 'banging': 568,\n",
       " 'bangkok': 569,\n",
       " 'banjo': 570,\n",
       " 'bank': 571,\n",
       " 'banking': 572,\n",
       " 'bankrupt': 573,\n",
       " 'bankruptcy': 574,\n",
       " 'banks': 575,\n",
       " 'banned': 576,\n",
       " 'baptism': 577,\n",
       " 'bar': 578,\n",
       " 'barbarians': 579,\n",
       " 'barbaric': 580,\n",
       " 'barbecue': 581,\n",
       " 'barber': 582,\n",
       " 'barbers': 583,\n",
       " 'bare': 584,\n",
       " 'barefoot': 585,\n",
       " 'barely': 586,\n",
       " 'bargain': 587,\n",
       " 'bark': 588,\n",
       " 'barking': 589,\n",
       " 'barks': 590,\n",
       " 'barn': 591,\n",
       " 'barricaded': 592,\n",
       " 'bars': 593,\n",
       " 'bartender': 594,\n",
       " 'bartolomeo': 595,\n",
       " 'baseball': 596,\n",
       " 'based': 597,\n",
       " 'basement': 598,\n",
       " 'bashful': 599,\n",
       " 'basho': 600,\n",
       " 'basic': 601,\n",
       " 'basically': 602,\n",
       " 'basket': 603,\n",
       " 'basketball': 604,\n",
       " 'bassoon': 605,\n",
       " 'bat': 606,\n",
       " 'batching': 607,\n",
       " 'bath': 608,\n",
       " 'bathing': 609,\n",
       " 'bathroom': 610,\n",
       " 'bathrooms': 611,\n",
       " 'bathtub': 612,\n",
       " 'bats': 613,\n",
       " 'batteries': 614,\n",
       " 'battery': 615,\n",
       " 'battle': 616,\n",
       " 'bazaar': 617,\n",
       " 'be': 618,\n",
       " 'beach': 619,\n",
       " 'beans': 620,\n",
       " 'bear': 621,\n",
       " 'beard': 622,\n",
       " 'beards': 623,\n",
       " 'bears': 624,\n",
       " 'beat': 625,\n",
       " 'beaten': 626,\n",
       " 'beating': 627,\n",
       " 'beatle': 628,\n",
       " 'beatles': 629,\n",
       " 'beats': 630,\n",
       " 'beautiful': 631,\n",
       " 'beautifully': 632,\n",
       " 'beauty': 633,\n",
       " 'became': 634,\n",
       " 'because': 635,\n",
       " 'beckoned': 636,\n",
       " 'become': 637,\n",
       " 'becomes': 638,\n",
       " 'becoming': 639,\n",
       " 'bed': 640,\n",
       " 'bedroom': 641,\n",
       " 'bedside': 642,\n",
       " 'bedtime': 643,\n",
       " 'bee': 644,\n",
       " 'beef': 645,\n",
       " 'been': 646,\n",
       " 'beep': 647,\n",
       " 'beer': 648,\n",
       " 'beers': 649,\n",
       " 'bees': 650,\n",
       " 'beethoven': 651,\n",
       " 'before': 652,\n",
       " 'beforehand': 653,\n",
       " 'beg': 654,\n",
       " 'began': 655,\n",
       " 'beggar': 656,\n",
       " 'beggars': 657,\n",
       " 'begged': 658,\n",
       " 'begin': 659,\n",
       " 'beginner': 660,\n",
       " 'beginners': 661,\n",
       " 'beginning': 662,\n",
       " 'begins': 663,\n",
       " 'begun': 664,\n",
       " 'behalf': 665,\n",
       " 'behave': 666,\n",
       " 'behaved': 667,\n",
       " 'behaving': 668,\n",
       " 'behavior': 669,\n",
       " 'behind': 670,\n",
       " 'beholder': 671,\n",
       " 'beijing': 672,\n",
       " 'being': 673,\n",
       " 'belated': 674,\n",
       " 'belgium': 675,\n",
       " 'belgrade': 676,\n",
       " 'belief': 677,\n",
       " 'believable': 678,\n",
       " 'believe': 679,\n",
       " 'believed': 680,\n",
       " 'believes': 681,\n",
       " 'believing': 682,\n",
       " 'bell': 683,\n",
       " 'belligerent': 684,\n",
       " 'bells': 685,\n",
       " 'belly': 686,\n",
       " 'belong': 687,\n",
       " 'belonged': 688,\n",
       " 'belongings': 689,\n",
       " 'belongs': 690,\n",
       " 'below': 691,\n",
       " 'belt': 692,\n",
       " 'belts': 693,\n",
       " 'bench': 694,\n",
       " 'beneficial': 695,\n",
       " 'benjamin': 696,\n",
       " 'bent': 697,\n",
       " 'berlin': 698,\n",
       " 'beside': 699,\n",
       " 'besides': 700,\n",
       " 'besieged': 701,\n",
       " 'best': 702,\n",
       " 'bet': 703,\n",
       " 'betray': 704,\n",
       " 'betrayed': 705,\n",
       " 'betraying': 706,\n",
       " 'better': 707,\n",
       " 'between': 708,\n",
       " 'beverages': 709,\n",
       " 'beware': 710,\n",
       " 'beyond': 711,\n",
       " 'bias': 712,\n",
       " 'biased': 713,\n",
       " 'bible': 714,\n",
       " 'bickering': 715,\n",
       " 'bicycle': 716,\n",
       " 'bicycles': 717,\n",
       " 'bieber': 718,\n",
       " 'big': 719,\n",
       " 'bigamy': 720,\n",
       " 'bigger': 721,\n",
       " 'biggest': 722,\n",
       " 'bigot': 723,\n",
       " 'bike': 724,\n",
       " 'bikers': 725,\n",
       " 'biking': 726,\n",
       " 'bikini': 727,\n",
       " 'bill': 728,\n",
       " 'billiards': 729,\n",
       " 'billion': 730,\n",
       " 'bills': 731,\n",
       " 'binge': 732,\n",
       " 'biographies': 733,\n",
       " 'biography': 734,\n",
       " 'biology': 735,\n",
       " 'birch': 736,\n",
       " 'bird': 737,\n",
       " 'birds': 738,\n",
       " 'birdwatching': 739,\n",
       " 'birth': 740,\n",
       " 'birthday': 741,\n",
       " 'birthdays': 742,\n",
       " 'birthrate': 743,\n",
       " 'bit': 744,\n",
       " 'bite': 745,\n",
       " 'bites': 746,\n",
       " 'biting': 747,\n",
       " 'bitten': 748,\n",
       " 'bitter': 749,\n",
       " 'bitterly': 750,\n",
       " 'biwa': 751,\n",
       " 'björks': 752,\n",
       " 'black': 753,\n",
       " 'blackboard': 754,\n",
       " 'blacked': 755,\n",
       " 'blackmailed': 756,\n",
       " 'blacks': 757,\n",
       " 'blade': 758,\n",
       " 'blame': 759,\n",
       " 'blamed': 760,\n",
       " 'blameless': 761,\n",
       " 'blames': 762,\n",
       " 'blaming': 763,\n",
       " 'blank': 764,\n",
       " 'blanket': 765,\n",
       " 'blanks': 766,\n",
       " 'blast': 767,\n",
       " 'blaze': 768,\n",
       " 'bleeding': 769,\n",
       " 'blender': 770,\n",
       " 'blessed': 771,\n",
       " 'blessing': 772,\n",
       " 'blew': 773,\n",
       " 'blind': 774,\n",
       " 'blinded': 775,\n",
       " 'blindfold': 776,\n",
       " 'blindfolded': 777,\n",
       " 'blinds': 778,\n",
       " 'blinked': 779,\n",
       " 'bliss': 780,\n",
       " 'bloated': 781,\n",
       " 'block': 782,\n",
       " 'blocked': 783,\n",
       " 'blog': 784,\n",
       " 'blonde': 785,\n",
       " 'blondes': 786,\n",
       " 'blood': 787,\n",
       " 'bloody': 788,\n",
       " 'bloom': 789,\n",
       " 'blooming': 790,\n",
       " 'blossom': 791,\n",
       " 'blossomed': 792,\n",
       " 'blossoms': 793,\n",
       " 'blouse': 794,\n",
       " 'blouses': 795,\n",
       " 'blowing': 796,\n",
       " 'blue': 797,\n",
       " 'blues': 798,\n",
       " 'bluffing': 799,\n",
       " 'blurry': 800,\n",
       " 'blurted': 801,\n",
       " 'blush': 802,\n",
       " 'blushed': 803,\n",
       " 'blushing': 804,\n",
       " 'bmw': 805,\n",
       " 'board': 806,\n",
       " 'boarded': 807,\n",
       " 'boarding': 808,\n",
       " 'boasts': 809,\n",
       " 'boat': 810,\n",
       " 'boats': 811,\n",
       " 'bodies': 812,\n",
       " 'body': 813,\n",
       " 'bodyguard': 814,\n",
       " 'bodyguards': 815,\n",
       " 'boil': 816,\n",
       " 'boiled': 817,\n",
       " 'boiling': 818,\n",
       " 'bold': 819,\n",
       " 'bolted': 820,\n",
       " 'bomb': 821,\n",
       " 'bonaparte': 822,\n",
       " 'bone': 823,\n",
       " 'bones': 824,\n",
       " 'bonkers': 825,\n",
       " 'bonus': 826,\n",
       " 'book': 827,\n",
       " 'booked': 828,\n",
       " 'bookkeeping': 829,\n",
       " 'books': 830,\n",
       " 'bookshelf': 831,\n",
       " 'bookshop': 832,\n",
       " 'bookstore': 833,\n",
       " 'bookworm': 834,\n",
       " 'boots': 835,\n",
       " 'border': 836,\n",
       " 'borders': 837,\n",
       " 'bore': 838,\n",
       " 'bored': 839,\n",
       " 'bores': 840,\n",
       " 'boring': 841,\n",
       " 'born': 842,\n",
       " 'borrow': 843,\n",
       " 'borrowed': 844,\n",
       " 'borrows': 845,\n",
       " 'bosom': 846,\n",
       " 'boss': 847,\n",
       " 'bosss': 848,\n",
       " 'bossy': 849,\n",
       " 'boston': 850,\n",
       " 'both': 851,\n",
       " 'bother': 852,\n",
       " 'bothering': 853,\n",
       " 'bottle': 854,\n",
       " 'bottled': 855,\n",
       " 'bottles': 856,\n",
       " 'bottom': 857,\n",
       " 'bought': 858,\n",
       " 'boulder': 859,\n",
       " 'bound': 860,\n",
       " 'bounds': 861,\n",
       " 'bouquet': 862,\n",
       " 'bourbon': 863,\n",
       " 'bow': 864,\n",
       " 'bowed': 865,\n",
       " 'bowl': 866,\n",
       " 'bowling': 867,\n",
       " 'bowls': 868,\n",
       " 'box': 869,\n",
       " 'boxes': 870,\n",
       " 'boy': 871,\n",
       " 'boyfriend': 872,\n",
       " 'boyfriends': 873,\n",
       " 'boys': 874,\n",
       " 'brace': 875,\n",
       " 'bracelet': 876,\n",
       " 'braces': 877,\n",
       " 'brad': 878,\n",
       " 'brain': 879,\n",
       " 'brains': 880,\n",
       " 'brake': 881,\n",
       " 'brakes': 882,\n",
       " 'branch': 883,\n",
       " 'branches': 884,\n",
       " 'brand': 885,\n",
       " 'brands': 886,\n",
       " 'brandy': 887,\n",
       " 'brass': 888,\n",
       " 'brave': 889,\n",
       " 'bravely': 890,\n",
       " 'brazil': 891,\n",
       " 'brazilian': 892,\n",
       " 'breach': 893,\n",
       " 'bread': 894,\n",
       " 'breads': 895,\n",
       " 'break': 896,\n",
       " 'breakdown': 897,\n",
       " 'breakfast': 898,\n",
       " 'breaks': 899,\n",
       " 'breast': 900,\n",
       " 'breastfed': 901,\n",
       " 'breastfeed': 902,\n",
       " 'breastfeeding': 903,\n",
       " 'breath': 904,\n",
       " 'breathe': 905,\n",
       " 'breathing': 906,\n",
       " 'breeds': 907,\n",
       " 'breeze': 908,\n",
       " 'brewing': 909,\n",
       " 'bribe': 910,\n",
       " 'bribes': 911,\n",
       " 'bricks': 912,\n",
       " 'bride': 913,\n",
       " 'bridge': 914,\n",
       " 'bridges': 915,\n",
       " 'briefcase': 916,\n",
       " 'briefing': 917,\n",
       " 'briefly': 918,\n",
       " 'bright': 919,\n",
       " 'brightened': 920,\n",
       " 'brightly': 921,\n",
       " 'brilliant': 922,\n",
       " 'bring': 923,\n",
       " 'bringing': 924,\n",
       " 'brings': 925,\n",
       " 'britain': 926,\n",
       " 'british': 927,\n",
       " 'broach': 928,\n",
       " 'broadcast': 929,\n",
       " 'broadens': 930,\n",
       " 'broccoli': 931,\n",
       " 'broke': 932,\n",
       " 'broken': 933,\n",
       " 'bronze': 934,\n",
       " 'broom': 935,\n",
       " 'brother': 936,\n",
       " 'brotherinlaw': 937,\n",
       " 'brothers': 938,\n",
       " 'brought': 939,\n",
       " 'brown': 940,\n",
       " 'browser': 941,\n",
       " 'bruise': 942,\n",
       " 'bruises': 943,\n",
       " 'brunettes': 944,\n",
       " 'brush': 945,\n",
       " 'brushed': 946,\n",
       " 'brushes': 947,\n",
       " 'brussels': 948,\n",
       " 'brutally': 949,\n",
       " 'bucharest': 950,\n",
       " 'buck': 951,\n",
       " 'bucket': 952,\n",
       " 'bucks': 953,\n",
       " 'bud': 954,\n",
       " 'budapest': 955,\n",
       " 'budge': 956,\n",
       " 'budget': 957,\n",
       " 'buenos': 958,\n",
       " 'bugged': 959,\n",
       " 'bugs': 960,\n",
       " 'build': 961,\n",
       " 'building': 962,\n",
       " 'buildings': 963,\n",
       " 'built': 964,\n",
       " 'bulb': 965,\n",
       " 'bulgaria': 966,\n",
       " 'bulk': 967,\n",
       " 'bull': 968,\n",
       " 'bullet': 969,\n",
       " 'bulletproof': 970,\n",
       " 'bullets': 971,\n",
       " 'bullfight': 972,\n",
       " 'bullying': 973,\n",
       " 'bum': 974,\n",
       " 'bumped': 975,\n",
       " 'bumper': 976,\n",
       " 'bunch': 977,\n",
       " 'bundle': 978,\n",
       " 'burger': 979,\n",
       " 'burgers': 980,\n",
       " 'burglar': 981,\n",
       " 'burglars': 982,\n",
       " 'buried': 983,\n",
       " 'burn': 984,\n",
       " 'burned': 985,\n",
       " 'burning': 986,\n",
       " 'burns': 987,\n",
       " 'burnt': 988,\n",
       " 'burped': 989,\n",
       " 'burst': 990,\n",
       " 'bus': 991,\n",
       " 'buses': 992,\n",
       " 'bush': 993,\n",
       " 'bushed': 994,\n",
       " 'bushes': 995,\n",
       " 'busier': 996,\n",
       " 'busiest': 997,\n",
       " 'business': 998,\n",
       " 'businessman': 999,\n",
       " 'businessmen': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create token to word dicitonary for both source and target\n",
    "\n",
    "reverse_source_char_index = dict((i,word) for word, i in source_token_index.items())\n",
    "reverse_target_char_index = dict((i,word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28427"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num_decoder_tokens = num_decoder_tokens*1.05\n",
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34829,), (3870,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train - Test Split\n",
    "X, y = txt.eng, txt.pol\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save the train and test dataframes for reproducing the results later, as they are shuffled.\n",
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            #matrix of zeros 128*47\n",
    "            encoder_source_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            #matrix of zeros 128*38\n",
    "            decoder_source_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            #matrix of zeros 128*38*28427\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            \n",
    "            \n",
    "            for i, (source_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                #Enumerate() method adds a counter to an iterable and returns it in a form of enumerate object.\n",
    "                for t, word in enumerate(source_text.split()):\n",
    "                    \n",
    "                    encoder_source_data[i, t] = source_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_source_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_source_data, decoder_source_data], decoder_target_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Decoder Model Architure\n",
    "\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Image(retina=True, filename='train_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34829, 3870)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "\n",
    "train_samples, val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "348/348 [==============================] - 1506s 4s/step - loss: 0.8418 - acc: 0.0321 - val_loss: 0.8555 - val_acc: 0.0428\n",
      "Epoch 2/50\n",
      "348/348 [==============================] - 2370s 7s/step - loss: 0.7846 - acc: 0.0530 - val_loss: 0.8849 - val_acc: 0.0649\n",
      "Epoch 3/50\n",
      "322/348 [==========================>...] - ETA: 2:06 - loss: 0.7501 - acc: 0.0796"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('nmt_weights.h5')q\n",
    "model.load_weights('nmt_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFERENCE SET UP\n",
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Polish Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Polish Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Polish Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Polish Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Polish Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Polish Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to take user input??\n",
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "print('Input English sentence:', 'My name is Paulina')\n",
    "print('Actual Polish Translation:', 'Mam na imie Paulina')\n",
    "print('Predicted Polish Translation:', decoded_sentence[:-4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
